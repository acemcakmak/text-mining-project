{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NERC with Fine-tuned Transformer Model using Large BERT \n",
    "\n",
    "We loaded the transformer model '[bert-large-NER](https://huggingface.co/dslim/bert-large-NER)' from the Hugging face repository, which is fine-tuned for Named Entity recognition.\n",
    "\n",
    "The model is trained on the [CoNLL-2003](https://aclanthology.org/W03-0419/) database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sinemis/anaconda3/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from simpletransformers.ner import NERModel\n",
    "from sklearn.metrics import classification_report\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the test data into proper form for BERT\n",
    "\n",
    "ner_test_data = pd.read_csv(\"NER-test.tsv\", sep=\"\\t\")\n",
    "\n",
    "test_sentences = []\n",
    "current_sentence = []\n",
    "for index, row in ner_test_data.iterrows():\n",
    "    if row[\"token id\"] == 0:\n",
    "        if current_sentence:\n",
    "            test_sentences.append(\" \".join(current_sentence))\n",
    "        current_sentence = []\n",
    "    current_sentence.append(row[\"token\"])\n",
    "\n",
    "if current_sentence:\n",
    "    test_sentences.append(\" \".join(current_sentence))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ORG', 'O', 'O', 'O', 'O', 'O', 'B-PERSON', 'I-PERSON', 'O', 'B-PERSON', 'I-PERSON', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ORG', 'I-ORG', 'I-ORG', 'I-ORG', 'I-ORG', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ORG', 'I-ORG', 'I-ORG', 'O', 'O', 'O', 'O', 'O', 'B-PERSON', 'O', 'O', 'B-WORK_OF_ART', 'I-WORK_OF_ART', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-PER', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-PER', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-WORK_OF_ART', 'I-WORK_OF_ART', 'I-WORK_OF_ART', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-DATE', 'I-DATE', 'O', 'O', 'O', 'O', 'O', 'O', 'B-WORK_OF_ART', 'I-WORK_OF_ART', 'I-WORK_OF_ART', 'I-WORK_OF_ART', 'I-WORK_OF_ART', 'O', 'B-PER', 'I-PER', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-WORK_OF_ART', 'I-WORK_OF_ART', 'I-WORK_OF_ART', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n"
     ]
    }
   ],
   "source": [
    "# Getting the ground truth labels\n",
    "ground_truth_labels = list(ner_test_data[\"BIO NER tag\"])\n",
    "print(ground_truth_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at dslim/bert-large-NER were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# init NERModel\n",
    "NAUR_model = NERModel(\n",
    "    model_type=\"bert\",\n",
    "    model_name=\"dslim/bert-large-NER\",\n",
    "    use_cuda=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:03<00:00,  3.25s/it]\n",
      "Running Prediction: 100%|██████████| 1/1 [00:02<00:00,  2.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[{'I': 'O'}, {'would': 'O'}, {\"n't\": 'O'}, {'be': 'O'}, {'caught': 'O'}, {'dead': 'O'}, {'watching': 'O'}, {'the': 'O'}, {'NFL': 'B-ORG'}, {'if': 'O'}, {'it': 'O'}, {'were': 'O'}, {\"n't\": 'O'}, {'for': 'O'}, {'Taylor': 'B-PER'}, {'Swift': 'I-PER'}, {'.': 'O'}], [{'Chris': 'B-PER'}, {\"O'Donnell\": 'I-PER'}, {'stated': 'O'}, {'that': 'O'}, {'while': 'O'}, {'filming': 'O'}, {'for': 'O'}, {'this': 'O'}, {'movie': 'O'}, {',': 'O'}, {'he': 'O'}, {'felt': 'O'}, {'like': 'O'}, {'he': 'O'}, {'was': 'O'}, {'in': 'O'}, {'a': 'O'}, {'Toys': 'B-MISC'}, {\"''\": 'I-ORG'}, {'R': 'I-MISC'}, {\"''\": 'I-ORG'}, {'Us': 'I-ORG'}, {'commercial': 'O'}, {'.': 'O'}], [{'The': 'O'}, {'whole': 'O'}, {'game': 'O'}, {'was': 'O'}, {'a': 'O'}, {'rollercoaster': 'O'}, {'ride': 'O'}, {',': 'O'}, {'but': 'O'}, {'Los': 'B-ORG'}, {'Angeles': 'I-ORG'}, {'Lakers': 'I-ORG'}, {'ultimately': 'O'}, {'persevered': 'O'}, {'and': 'O'}, {'won': 'O'}, {'!': 'O'}], [{'Zendaya': 'B-PER'}, {'slayed': 'O'}, {'in': 'O'}, {'Dune': 'B-MISC'}, {'2': 'I-MISC'}, {',': 'O'}, {'as': 'O'}, {'she': 'O'}, {'does': 'O'}, {'in': 'O'}, {'all': 'O'}, {'her': 'O'}, {'movies': 'O'}, {'.': 'O'}], [{'While': 'O'}, {'my': 'O'}, {'favorite': 'O'}, {'player': 'O'}, {'was': 'O'}, {'playing': 'O'}, {'this': 'O'}, {'match': 'O'}, {'and': 'O'}, {'started': 'O'}, {'off': 'O'}, {'strongggg': 'O'}, {',': 'O'}, {'it': 'O'}, {'went': 'O'}, {'downhill': 'O'}, {'after': 'O'}, {'Messi': 'B-PER'}, {\"'s\": 'O'}, {'injyry': 'O'}, {'midgame': 'O'}, {'.': 'O'}], [{'My': 'O'}, {'uncle': 'O'}, {\"'s\": 'O'}, {'brother': 'O'}, {\"'s\": 'O'}, {'neighbor': 'O'}, {\"'s\": 'O'}, {'cat': 'O'}, {\"'s\": 'O'}, {'veterinarian': 'O'}, {'David': 'B-PER'}, {'reads': 'O'}, {'the': 'O'}, {'communist': 'O'}, {'manifesto': 'O'}, {'in': 'O'}, {'his': 'O'}, {'spare': 'O'}, {'time': 'O'}, {'.': 'O'}], [{'He': 'O'}, {'said': 'O'}, {'that': 'O'}, {'The': 'B-MISC'}, {'Great': 'I-MISC'}, {'Gatsby': 'I-MISC'}, {'is': 'O'}, {'the': 'O'}, {'best': 'O'}, {'novell': 'O'}, {'ever': 'O'}, {',': 'O'}, {'and': 'O'}, {'I': 'O'}, {'was': 'O'}, {'about': 'O'}, {'to': 'O'}, {'throw': 'O'}, {'hands': 'O'}, {'.': 'O'}], [{'I': 'O'}, {'could': 'O'}, {'not': 'O'}, {'look': 'O'}, {'away': 'O'}, {'from': 'O'}, {'this': 'O'}, {'train': 'O'}, {'wrck': 'O'}, {'of': 'O'}, {'a': 'O'}, {'movie': 'O'}, {',': 'O'}, {'on': 'O'}, {'February': 'O'}, {'14th': 'O'}, {'of': 'O'}, {'all': 'O'}, {'days': 'O'}, {'.': 'O'}], [{'The': 'O'}, {'film': 'O'}, {'Everything': 'B-MISC'}, {'Everywhere': 'I-MISC'}, {'All': 'I-MISC'}, {'At': 'I-MISC'}, {'Once': 'I-MISC'}, {'follows': 'O'}, {'Evelyn': 'B-PER'}, {'Wang': 'I-PER'}, {',': 'O'}, {'a': 'O'}, {'woman': 'O'}, {'drowning': 'O'}, {'under': 'O'}, {'the': 'O'}, {'stress': 'O'}, {'of': 'O'}, {'her': 'O'}, {'family': 'O'}, {\"'s\": 'O'}, {'failing': 'O'}, {'laundromat': 'O'}, {'.': 'O'}], [{'I': 'O'}, {'just': 'O'}, {'finished': 'O'}, {'reading': 'O'}, {'pride': 'O'}, {'and': 'O'}, {'prejudice': 'O'}, {'which': 'O'}, {'had': 'O'}, {'me': 'O'}, {'HOOOKED': 'O'}, {'from': 'O'}, {'the': 'O'}, {'beginning': 'O'}, {'.': 'O'}]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Predictions\n",
    "predictions, _ = NAUR_model.predict(test_sentences)\n",
    "\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ORG', 'O', 'O', 'O', 'O', 'O', 'B-PER', 'I-PER', 'O', 'B-PER', 'I-PER', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-MISC', 'I-ORG', 'I-MISC', 'I-ORG', 'I-ORG', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ORG', 'I-ORG', 'I-ORG', 'O', 'O', 'O', 'O', 'O', 'B-PER', 'O', 'O', 'B-MISC', 'I-MISC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-PER', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-PER', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-MISC', 'I-MISC', 'I-MISC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-MISC', 'I-MISC', 'I-MISC', 'I-MISC', 'I-MISC', 'O', 'B-PER', 'I-PER', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n"
     ]
    }
   ],
   "source": [
    "# Need to get a list of only predicted labels for evaluation part\n",
    "just_labels_list = []\n",
    "\n",
    "for sentence_labels in predictions:\n",
    "    for token_labels in sentence_labels:\n",
    "        #print(token_labels)\n",
    "        labella = next(iter(token_labels.values()))\n",
    "        #print(only_value)\n",
    "        just_labels_list.append(labella)\n",
    "\n",
    "\n",
    "print(just_labels_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               precision    recall  f1-score   support\n",
      "\n",
      "       B-DATE       0.00      0.00      0.00         1\n",
      "       B-MISC       0.00      0.00      0.00         0\n",
      "        B-ORG       1.00      0.67      0.80         3\n",
      "        B-PER       0.50      1.00      0.67         3\n",
      "     B-PERSON       0.00      0.00      0.00         3\n",
      "B-WORK_OF_ART       0.00      0.00      0.00         4\n",
      "       I-DATE       0.00      0.00      0.00         1\n",
      "       I-MISC       0.00      0.00      0.00         0\n",
      "        I-ORG       1.00      0.83      0.91         6\n",
      "        I-PER       0.33      1.00      0.50         1\n",
      "     I-PERSON       0.00      0.00      0.00         2\n",
      "I-WORK_OF_ART       0.00      0.00      0.00         9\n",
      "            O       0.97      1.00      0.98       160\n",
      "\n",
      "     accuracy                           0.89       193\n",
      "    macro avg       0.29      0.35      0.30       193\n",
      " weighted avg       0.86      0.89      0.87       193\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sinemis/anaconda3/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/sinemis/anaconda3/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/sinemis/anaconda3/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/sinemis/anaconda3/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/sinemis/anaconda3/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/sinemis/anaconda3/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "# Evaluation\n",
    "report = classification_report(ground_truth_labels, just_labels_list)\n",
    "print(report)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
