{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NERC using Flair\n",
    "\n",
    "We used [Flair](https://github.com/flairNLP/flair/) library's SequenceTagger model for NERC. This model was pre-trained on the [OntoNotes 5.0](https://paperswithcode.com/dataset/ontonotes-5-0) corpus.\n",
    "\n",
    "\n",
    "https://huggingface.co/flair/ner-english-ontonotes-large"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "@misc{schweter2020flert,\n",
    "    title={FLERT: Document-Level Features for Named Entity Recognition},\n",
    "    author={Stefan Schweter and Alan Akbik},\n",
    "    year={2020},\n",
    "    eprint={2011.06993},\n",
    "    archivePrefix={arXiv},\n",
    "    primaryClass={cs.CL}\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install flair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sinemis/anaconda3/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from flair.data import Sentence\n",
    "from flair.models import SequenceTagger\n",
    "from sklearn.metrics import classification_report\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"I would n't be caught dead watching the NFL if it were n't for Taylor Swift.\", \"Chris O'Donnell stated that while filming for this movie , he felt like he was in a Toys '' R '' Us commercial.\", 'The whole game was a rollercoaster ride , but Los Angeles Lakers ultimately persevered and won!', 'Zendaya slayed in Dune 2 , as she does in all her movies.', \"While my favorite player was playing this match and started off strongggg , it went downhill after Messi 's injyry midgame.\", \"My uncle 's brother 's neighbor 's cat 's veterinarian David reads the communist manifesto in his spare time.\", 'He said that The Great Gatsby is the best novell ever , and I was about to throw hands.', 'I could not look away from this train wrck of a movie , on February 14th of all days.', \"The film Everything Everywhere All At Once follows Evelyn Wang , a woman drowning under the stress of her family 's failing laundromat.\", 'I just finished reading pride and prejudice which had me HOOOKED from the beginning.']\n"
     ]
    }
   ],
   "source": [
    "# Getting the test data into proper form to sentence level\n",
    "\n",
    "ner_test_data = pd.read_csv(\"NER-test.tsv\", sep=\"\\t\")\n",
    "\n",
    "test_sentences = []\n",
    "current_sentence = []\n",
    "for index, row in ner_test_data.iterrows():\n",
    "    if row[\"token id\"] == 0:\n",
    "        if current_sentence:\n",
    "            s = \" \".join(current_sentence)\n",
    "            s =  s[:-2] + s[-1] # To delete the space from the second to last index that occurs from the previos operation\n",
    "            test_sentences.append(s)\n",
    "        current_sentence = []\n",
    "    current_sentence.append(row[\"token\"])\n",
    "\n",
    "if current_sentence:\n",
    "    s = \" \".join(current_sentence)\n",
    "    s =  s[:-2] + s[-1]\n",
    "    test_sentences.append(s)\n",
    "\n",
    "\n",
    "print(test_sentences)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert flair NE tags to IOB format \n",
    "\n",
    "def iob_adder_for_flair(sentence):\n",
    "\n",
    "    spans = sentence.get_spans('ner')\n",
    "    iob_labels = []\n",
    "    start_O_idx = 0\n",
    "\n",
    "    for entity in spans:\n",
    "        inside = False\n",
    "        end_O_idx = entity[0].idx - 1\n",
    "\n",
    "        #print(f\"Entity:, {entity}\")\n",
    "        #print(f\"Text:, {entity.text}\")\n",
    "        label = entity.tag\n",
    "        \n",
    "        for i in range(start_O_idx, end_O_idx):\n",
    "            iob_labels.append('O')\n",
    "            print(f\"{sentence[i]} -> 'O\")  \n",
    "        \n",
    "        start_O_idx= entity[-1].idx\n",
    "\n",
    "        for token in entity:\n",
    "            current_idx = token.idx - 1\n",
    "\n",
    "            if inside:\n",
    "                iob_labels.append('I-' + label)\n",
    "            else:\n",
    "                iob_labels.append('B-' + label)\n",
    "                inside = True\n",
    "            \n",
    "            print(f\"{sentence[current_idx]} -> {label}\")  \n",
    "\n",
    "\n",
    "            \n",
    "    sent_length = len(sentence) - 1\n",
    "    if start_O_idx != sent_length:\n",
    "        for i in range(start_O_idx, sent_length):\n",
    "            iob_labels.append('O')\n",
    "            print(f\"{sentence[i]} -> 'O\") \n",
    "\n",
    "\n",
    "    return iob_labels\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-03-31 20:43:47,453 SequenceTagger predicts: Dictionary with 76 tags: <unk>, O, B-CARDINAL, E-CARDINAL, S-PERSON, S-CARDINAL, S-PRODUCT, B-PRODUCT, I-PRODUCT, E-PRODUCT, B-WORK_OF_ART, I-WORK_OF_ART, E-WORK_OF_ART, B-PERSON, E-PERSON, S-GPE, B-DATE, I-DATE, E-DATE, S-ORDINAL, S-LANGUAGE, I-PERSON, S-EVENT, S-DATE, B-QUANTITY, E-QUANTITY, S-TIME, B-TIME, I-TIME, E-TIME, B-GPE, E-GPE, S-ORG, I-GPE, S-NORP, B-FAC, I-FAC, E-FAC, B-NORP, E-NORP, S-PERCENT, B-ORG, E-ORG, B-LANGUAGE, E-LANGUAGE, I-CARDINAL, I-ORG, S-WORK_OF_ART, I-QUANTITY, B-MONEY\n",
      "Token[0]: \"I\" -> 'O\n",
      "Token[1]: \"would\" -> 'O\n",
      "Token[2]: \"n't\" -> 'O\n",
      "Token[3]: \"be\" -> 'O\n",
      "Token[4]: \"caught\" -> 'O\n",
      "Token[5]: \"dead\" -> 'O\n",
      "Token[6]: \"watching\" -> 'O\n",
      "Token[7]: \"the\" -> 'O\n",
      "Token[8]: \"NFL\" -> ORG\n",
      "Token[9]: \"if\" -> 'O\n",
      "Token[10]: \"it\" -> 'O\n",
      "Token[11]: \"were\" -> 'O\n",
      "Token[12]: \"n't\" -> 'O\n",
      "Token[13]: \"for\" -> 'O\n",
      "Token[14]: \"Taylor\" -> PERSON\n",
      "Token[15]: \"Swift\" -> PERSON\n",
      "Token[0]: \"Chris\" -> PERSON\n",
      "Token[1]: \"O'Donnell\" -> PERSON\n",
      "Token[2]: \"stated\" -> 'O\n",
      "Token[3]: \"that\" -> 'O\n",
      "Token[4]: \"while\" -> 'O\n",
      "Token[5]: \"filming\" -> 'O\n",
      "Token[6]: \"for\" -> 'O\n",
      "Token[7]: \"this\" -> 'O\n",
      "Token[8]: \"movie\" -> 'O\n",
      "Token[9]: \",\" -> 'O\n",
      "Token[10]: \"he\" -> 'O\n",
      "Token[11]: \"felt\" -> 'O\n",
      "Token[12]: \"like\" -> 'O\n",
      "Token[13]: \"he\" -> 'O\n",
      "Token[14]: \"was\" -> 'O\n",
      "Token[15]: \"in\" -> 'O\n",
      "Token[16]: \"a\" -> 'O\n",
      "Token[17]: \"Toys\" -> ORG\n",
      "Token[18]: \"''\" -> ORG\n",
      "Token[19]: \"R\" -> ORG\n",
      "Token[20]: \"''\" -> ORG\n",
      "Token[21]: \"Us\" -> ORG\n",
      "Token[22]: \"commercial\" -> 'O\n",
      "Token[0]: \"The\" -> 'O\n",
      "Token[1]: \"whole\" -> 'O\n",
      "Token[2]: \"game\" -> 'O\n",
      "Token[3]: \"was\" -> 'O\n",
      "Token[4]: \"a\" -> 'O\n",
      "Token[5]: \"rollercoaster\" -> 'O\n",
      "Token[6]: \"ride\" -> 'O\n",
      "Token[7]: \",\" -> 'O\n",
      "Token[8]: \"but\" -> 'O\n",
      "Token[9]: \"Los\" -> ORG\n",
      "Token[10]: \"Angeles\" -> ORG\n",
      "Token[11]: \"Lakers\" -> ORG\n",
      "Token[12]: \"ultimately\" -> 'O\n",
      "Token[13]: \"persevered\" -> 'O\n",
      "Token[14]: \"and\" -> 'O\n",
      "Token[15]: \"won\" -> 'O\n",
      "Token[0]: \"Zendaya\" -> PERSON\n",
      "Token[1]: \"slayed\" -> 'O\n",
      "Token[2]: \"in\" -> 'O\n",
      "Token[3]: \"Dune\" -> WORK_OF_ART\n",
      "Token[4]: \"2\" -> WORK_OF_ART\n",
      "Token[5]: \",\" -> 'O\n",
      "Token[6]: \"as\" -> 'O\n",
      "Token[7]: \"she\" -> 'O\n",
      "Token[8]: \"does\" -> 'O\n",
      "Token[9]: \"in\" -> 'O\n",
      "Token[10]: \"all\" -> 'O\n",
      "Token[11]: \"her\" -> 'O\n",
      "Token[12]: \"movies\" -> 'O\n",
      "Token[0]: \"While\" -> 'O\n",
      "Token[1]: \"my\" -> 'O\n",
      "Token[2]: \"favorite\" -> 'O\n",
      "Token[3]: \"player\" -> 'O\n",
      "Token[4]: \"was\" -> 'O\n",
      "Token[5]: \"playing\" -> 'O\n",
      "Token[6]: \"this\" -> 'O\n",
      "Token[7]: \"match\" -> 'O\n",
      "Token[8]: \"and\" -> 'O\n",
      "Token[9]: \"started\" -> 'O\n",
      "Token[10]: \"off\" -> 'O\n",
      "Token[11]: \"strongggg\" -> 'O\n",
      "Token[12]: \",\" -> 'O\n",
      "Token[13]: \"it\" -> 'O\n",
      "Token[14]: \"went\" -> 'O\n",
      "Token[15]: \"downhill\" -> 'O\n",
      "Token[16]: \"after\" -> 'O\n",
      "Token[17]: \"Messi\" -> PERSON\n",
      "Token[18]: \"'\" -> 'O\n",
      "Token[19]: \"s\" -> 'O\n",
      "Token[20]: \"injyry\" -> 'O\n",
      "Token[21]: \"midgame\" -> 'O\n",
      "Token[0]: \"My\" -> 'O\n",
      "Token[1]: \"uncle\" -> 'O\n",
      "Token[2]: \"'\" -> 'O\n",
      "Token[3]: \"s\" -> 'O\n",
      "Token[4]: \"brother\" -> 'O\n",
      "Token[5]: \"'\" -> 'O\n",
      "Token[6]: \"s\" -> 'O\n",
      "Token[7]: \"neighbor\" -> 'O\n",
      "Token[8]: \"'\" -> 'O\n",
      "Token[9]: \"s\" -> 'O\n",
      "Token[10]: \"cat\" -> 'O\n",
      "Token[11]: \"'\" -> 'O\n",
      "Token[12]: \"s\" -> 'O\n",
      "Token[13]: \"veterinarian\" -> 'O\n",
      "Token[14]: \"David\" -> PERSON\n",
      "Token[15]: \"reads\" -> 'O\n",
      "Token[16]: \"the\" -> 'O\n",
      "Token[17]: \"communist\" -> NORP\n",
      "Token[18]: \"manifesto\" -> 'O\n",
      "Token[19]: \"in\" -> 'O\n",
      "Token[20]: \"his\" -> 'O\n",
      "Token[21]: \"spare\" -> 'O\n",
      "Token[22]: \"time\" -> 'O\n",
      "Token[0]: \"He\" -> 'O\n",
      "Token[1]: \"said\" -> 'O\n",
      "Token[2]: \"that\" -> 'O\n",
      "Token[3]: \"The\" -> WORK_OF_ART\n",
      "Token[4]: \"Great\" -> WORK_OF_ART\n",
      "Token[5]: \"Gatsby\" -> WORK_OF_ART\n",
      "Token[6]: \"is\" -> 'O\n",
      "Token[7]: \"the\" -> 'O\n",
      "Token[8]: \"best\" -> 'O\n",
      "Token[9]: \"novell\" -> 'O\n",
      "Token[10]: \"ever\" -> 'O\n",
      "Token[11]: \",\" -> 'O\n",
      "Token[12]: \"and\" -> 'O\n",
      "Token[13]: \"I\" -> 'O\n",
      "Token[14]: \"was\" -> 'O\n",
      "Token[15]: \"about\" -> 'O\n",
      "Token[16]: \"to\" -> 'O\n",
      "Token[17]: \"throw\" -> 'O\n",
      "Token[18]: \"hands\" -> 'O\n",
      "Token[0]: \"I\" -> 'O\n",
      "Token[1]: \"could\" -> 'O\n",
      "Token[2]: \"not\" -> 'O\n",
      "Token[3]: \"look\" -> 'O\n",
      "Token[4]: \"away\" -> 'O\n",
      "Token[5]: \"from\" -> 'O\n",
      "Token[6]: \"this\" -> 'O\n",
      "Token[7]: \"train\" -> 'O\n",
      "Token[8]: \"wrck\" -> 'O\n",
      "Token[9]: \"of\" -> 'O\n",
      "Token[10]: \"a\" -> 'O\n",
      "Token[11]: \"movie\" -> 'O\n",
      "Token[12]: \",\" -> 'O\n",
      "Token[13]: \"on\" -> 'O\n",
      "Token[14]: \"February\" -> DATE\n",
      "Token[15]: \"14th\" -> DATE\n",
      "Token[16]: \"of\" -> DATE\n",
      "Token[17]: \"all\" -> DATE\n",
      "Token[18]: \"days\" -> DATE\n",
      "Token[0]: \"The\" -> 'O\n",
      "Token[1]: \"film\" -> 'O\n",
      "Token[2]: \"Everything\" -> WORK_OF_ART\n",
      "Token[3]: \"Everywhere\" -> WORK_OF_ART\n",
      "Token[4]: \"All\" -> WORK_OF_ART\n",
      "Token[5]: \"At\" -> WORK_OF_ART\n",
      "Token[6]: \"Once\" -> WORK_OF_ART\n",
      "Token[7]: \"follows\" -> 'O\n",
      "Token[8]: \"Evelyn\" -> PERSON\n",
      "Token[9]: \"Wang\" -> PERSON\n",
      "Token[10]: \",\" -> 'O\n",
      "Token[11]: \"a\" -> 'O\n",
      "Token[12]: \"woman\" -> 'O\n",
      "Token[13]: \"drowning\" -> 'O\n",
      "Token[14]: \"under\" -> 'O\n",
      "Token[15]: \"the\" -> 'O\n",
      "Token[16]: \"stress\" -> 'O\n",
      "Token[17]: \"of\" -> 'O\n",
      "Token[18]: \"her\" -> 'O\n",
      "Token[19]: \"family\" -> 'O\n",
      "Token[20]: \"'\" -> 'O\n",
      "Token[21]: \"s\" -> 'O\n",
      "Token[22]: \"failing\" -> 'O\n",
      "Token[23]: \"laundromat\" -> 'O\n",
      "Token[0]: \"I\" -> 'O\n",
      "Token[1]: \"just\" -> 'O\n",
      "Token[2]: \"finished\" -> 'O\n",
      "Token[3]: \"reading\" -> 'O\n",
      "Token[4]: \"pride\" -> WORK_OF_ART\n",
      "Token[5]: \"and\" -> WORK_OF_ART\n",
      "Token[6]: \"prejudice\" -> WORK_OF_ART\n",
      "Token[7]: \"which\" -> 'O\n",
      "Token[8]: \"had\" -> 'O\n",
      "Token[9]: \"me\" -> 'O\n",
      "Token[10]: \"HOOOKED\" -> 'O\n",
      "Token[11]: \"from\" -> 'O\n",
      "Token[12]: \"the\" -> 'O\n",
      "Token[13]: \"beginning\" -> 'O\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ORG', 'O', 'O', 'O', 'O', 'O', 'B-PERSON', 'I-PERSON', 'B-PERSON', 'I-PERSON', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ORG', 'I-ORG', 'I-ORG', 'I-ORG', 'I-ORG', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ORG', 'I-ORG', 'I-ORG', 'O', 'O', 'O', 'O', 'B-PERSON', 'O', 'O', 'B-WORK_OF_ART', 'I-WORK_OF_ART', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-PERSON', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-PERSON', 'O', 'O', 'B-NORP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-WORK_OF_ART', 'I-WORK_OF_ART', 'I-WORK_OF_ART', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-DATE', 'I-DATE', 'I-DATE', 'I-DATE', 'I-DATE', 'O', 'O', 'B-WORK_OF_ART', 'I-WORK_OF_ART', 'I-WORK_OF_ART', 'I-WORK_OF_ART', 'I-WORK_OF_ART', 'O', 'B-PERSON', 'I-PERSON', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-WORK_OF_ART', 'I-WORK_OF_ART', 'I-WORK_OF_ART', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n"
     ]
    }
   ],
   "source": [
    "seq_tagger = SequenceTagger.load(\"flair/ner-english-ontonotes-large\")\n",
    "# https://huggingface.co/flair/ner-english-ontonotes-large\n",
    "\n",
    "predictions = [] \n",
    "\n",
    "for sent in test_sentences:\n",
    "    flair_sentence = Sentence(sent)\n",
    "    seq_tagger.predict(flair_sentence)\n",
    "    predictions.extend( iob_adder_for_flair(flair_sentence) )\n",
    "\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ORG', 'O', 'O', 'O', 'O', 'O', 'B-PERSON', 'I-PERSON', 'O', 'B-PERSON', 'I-PERSON', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ORG', 'I-ORG', 'I-ORG', 'I-ORG', 'I-ORG', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ORG', 'I-ORG', 'I-ORG', 'O', 'O', 'O', 'O', 'O', 'B-PERSON', 'O', 'O', 'B-WORK_OF_ART', 'I-WORK_OF_ART', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-PER', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-PER', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-WORK_OF_ART', 'I-WORK_OF_ART', 'I-WORK_OF_ART', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-DATE', 'I-DATE', 'O', 'O', 'O', 'O', 'O', 'O', 'B-WORK_OF_ART', 'I-WORK_OF_ART', 'I-WORK_OF_ART', 'I-WORK_OF_ART', 'I-WORK_OF_ART', 'O', 'B-PER', 'I-PER', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-WORK_OF_ART', 'I-WORK_OF_ART', 'I-WORK_OF_ART', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n"
     ]
    }
   ],
   "source": [
    "# Getting the ground truth labels\n",
    "ground_truth_labels = list(ner_test_data[\"BIO NER tag\"])\n",
    "print(ground_truth_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               precision    recall  f1-score   support\n",
      "\n",
      "       B-DATE       0.00      0.00      0.00         1\n",
      "       B-NORP       0.00      0.00      0.00         0\n",
      "        B-ORG       0.33      0.33      0.33         3\n",
      "        B-PER       0.00      0.00      0.00         3\n",
      "     B-PERSON       0.17      0.33      0.22         3\n",
      "B-WORK_OF_ART       0.00      0.00      0.00         4\n",
      "       I-DATE       0.25      1.00      0.40         1\n",
      "        I-ORG       0.50      0.50      0.50         6\n",
      "        I-PER       0.00      0.00      0.00         1\n",
      "     I-PERSON       0.33      0.50      0.40         2\n",
      "I-WORK_OF_ART       0.22      0.22      0.22         9\n",
      "            O       0.91      0.89      0.90       160\n",
      "\n",
      "     accuracy                           0.78       193\n",
      "    macro avg       0.23      0.31      0.25       193\n",
      " weighted avg       0.79      0.78      0.79       193\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sinemis/anaconda3/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/sinemis/anaconda3/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/sinemis/anaconda3/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/sinemis/anaconda3/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/sinemis/anaconda3/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/sinemis/anaconda3/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "# Evaluation\n",
    "\n",
    "lenp = len(predictions)\n",
    "leng = len(ground_truth_labels)\n",
    "for _ in range(leng-lenp):\n",
    "    predictions.append('O')\n",
    "\n",
    "report = classification_report(ground_truth_labels, predictions)\n",
    "print(report)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
